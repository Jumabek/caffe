I1016 23:26:27.671274 26842 caffe.cpp:217] Using GPUs 0
I1016 23:26:27.678130 26842 caffe.cpp:222] GPU 0: Tesla K40c
I1016 23:26:28.021682 26842 solver.cpp:48] Initializing solver from parameters: 
test_iter: 1
test_interval: 50000
base_lr: 0.0001
display: 20
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
snapshot: 50000
snapshot_prefix: "models/bvlc_reference_caffenet_siamese_5conv/caffenet_train"
solver_mode: GPU
device_id: 0
net: "models/bvlc_reference_caffenet_siamese_5conv/train_val.prototxt"
train_state {
  level: 0
  stage: ""
}
I1016 23:26:28.021800 26842 solver.cpp:91] Creating training net from net file: models/bvlc_reference_caffenet_siamese_5conv/train_val.prototxt
I1016 23:26:28.022613 26842 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I1016 23:26:28.022639 26842 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer data_p
I1016 23:26:28.022858 26842 net.cpp:58] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/media/ailab/Data/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/media/ailab/Data/imagenet/ilsvrc12_train_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "conv5"
  top: "fc6"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    name: "fc8_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc8_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "data_p"
  type: "Data"
  top: "data_p"
  top: "label_p"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/media/ailab/Data/imagenet/imagenet_mean.binaryproto"
  }
  data_param {
    source: "/media/ailab/Data/imagenet/ilsvrc12_train2_lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "conv1_p"
  top: "conv1_p"
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1_p"
  type: "LRN"
  bottom: "pool1_p"
  top: "norm1_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "norm1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2_p"
  type: "ReLU"
  bottom: "conv2_p"
  top: "conv2_p"
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2_p"
  type: "LRN"
  bottom: "pool2_p"
  top: "norm2_p"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3_p"
  type: "Convolution"
  bottom: "norm2_p"
  top: "conv3_p"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3_p"
  type: "ReLU"
  bottom: "conv3_p"
  top: "conv3_p"
}
layer {
  name: "conv4_p"
  type: "Convolution"
  bottom: "conv3_p"
  top: "conv4_p"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4_p"
  type: "ReLU"
  bottom: "conv4_p"
  top: "conv4_p"
}
layer {
  name: "conv5_p"
  type: "Convolution"
  bottom: "conv4_p"
  top: "conv5_p"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5_p"
  type: "ReLU"
  bottom: "conv5_p"
  top: "conv5_p"
}
layer {
  name: "fc6_p"
  type: "InnerProduct"
  bottom: "conv5_p"
  top: "fc6_p"
  param {
    name: "fc6_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc6_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6_p"
  type: "ReLU"
  bottom: "fc6_p"
  top: "fc6_p"
}
layer {
  name: "drop6_p"
  type: "Dropout"
  bottom: "fc6_p"
  top: "fc6_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_p"
  type: "InnerProduct"
  bottom: "fc6_p"
  top: "fc7_p"
  param {
    name: "fc7_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc7_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7_p"
  type: "ReLU"
  bottom: "fc7_p"
  top: "fc7_p"
}
layer {
  name: "drop7_p"
  type: "Dropout"
  bottom: "fc7_p"
  top: "fc7_p"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_p"
  type: "InnerProduct"
  bottom: "fc7_p"
  top: "fc8_p"
  param {
    name: "fc8_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "fc8_b"
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1000
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "same_not_same_label"
  type: "Python"
  bottom: "label"
  bottom: "label_p"
  top: "same_not_same_label"
  python_param {
    module: "siamese"
    layer: "SiameseLabels"
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "fc8"
  bottom: "fc8_p"
  bottom: "same_not_same_label"
  top: "loss"
  contrastive_loss_param {
    margin: 5
  }
}
I1016 23:26:28.022996 26842 layer_factory.hpp:77] Creating layer data
I1016 23:26:28.023469 26842 net.cpp:100] Creating Layer data
I1016 23:26:28.023507 26842 net.cpp:408] data -> data
I1016 23:26:28.023527 26842 net.cpp:408] data -> label
I1016 23:26:28.023538 26842 data_transformer.cpp:25] Loading mean file from: /media/ailab/Data/imagenet/imagenet_mean.binaryproto
I1016 23:26:28.170192 26846 db_lmdb.cpp:35] Opened lmdb /media/ailab/Data/imagenet/ilsvrc12_train_lmdb
I1016 23:26:28.244282 26842 data_layer.cpp:41] output data size: 128,3,227,227
I1016 23:26:28.515533 26842 net.cpp:150] Setting up data
I1016 23:26:28.515568 26842 net.cpp:157] Top shape: 128 3 227 227 (19787136)
I1016 23:26:28.515574 26842 net.cpp:157] Top shape: 128 (128)
I1016 23:26:28.515578 26842 net.cpp:165] Memory required for data: 79149056
I1016 23:26:28.515594 26842 layer_factory.hpp:77] Creating layer conv1
I1016 23:26:28.515628 26842 net.cpp:100] Creating Layer conv1
I1016 23:26:28.515635 26842 net.cpp:434] conv1 <- data
I1016 23:26:28.515650 26842 net.cpp:408] conv1 -> conv1
I1016 23:26:28.572154 26842 net.cpp:150] Setting up conv1
I1016 23:26:28.572182 26842 net.cpp:157] Top shape: 128 96 55 55 (37171200)
I1016 23:26:28.572187 26842 net.cpp:165] Memory required for data: 227833856
I1016 23:26:28.572201 26842 layer_factory.hpp:77] Creating layer relu1
I1016 23:26:28.572211 26842 net.cpp:100] Creating Layer relu1
I1016 23:26:28.572216 26842 net.cpp:434] relu1 <- conv1
I1016 23:26:28.572221 26842 net.cpp:395] relu1 -> conv1 (in-place)
I1016 23:26:28.572229 26842 net.cpp:150] Setting up relu1
I1016 23:26:28.572233 26842 net.cpp:157] Top shape: 128 96 55 55 (37171200)
I1016 23:26:28.572237 26842 net.cpp:165] Memory required for data: 376518656
I1016 23:26:28.572239 26842 layer_factory.hpp:77] Creating layer pool1
I1016 23:26:28.572248 26842 net.cpp:100] Creating Layer pool1
I1016 23:26:28.572252 26842 net.cpp:434] pool1 <- conv1
I1016 23:26:28.572257 26842 net.cpp:408] pool1 -> pool1
I1016 23:26:28.572304 26842 net.cpp:150] Setting up pool1
I1016 23:26:28.572310 26842 net.cpp:157] Top shape: 128 96 27 27 (8957952)
I1016 23:26:28.572314 26842 net.cpp:165] Memory required for data: 412350464
I1016 23:26:28.572316 26842 layer_factory.hpp:77] Creating layer norm1
I1016 23:26:28.572324 26842 net.cpp:100] Creating Layer norm1
I1016 23:26:28.572327 26842 net.cpp:434] norm1 <- pool1
I1016 23:26:28.572331 26842 net.cpp:408] norm1 -> norm1
I1016 23:26:28.572360 26842 net.cpp:150] Setting up norm1
I1016 23:26:28.572365 26842 net.cpp:157] Top shape: 128 96 27 27 (8957952)
I1016 23:26:28.572367 26842 net.cpp:165] Memory required for data: 448182272
I1016 23:26:28.572371 26842 layer_factory.hpp:77] Creating layer conv2
I1016 23:26:28.572379 26842 net.cpp:100] Creating Layer conv2
I1016 23:26:28.572383 26842 net.cpp:434] conv2 <- norm1
I1016 23:26:28.572388 26842 net.cpp:408] conv2 -> conv2
I1016 23:26:28.580284 26842 net.cpp:150] Setting up conv2
I1016 23:26:28.580312 26842 net.cpp:157] Top shape: 128 256 23 23 (17334272)
I1016 23:26:28.580319 26842 net.cpp:165] Memory required for data: 517519360
I1016 23:26:28.580332 26842 layer_factory.hpp:77] Creating layer relu2
I1016 23:26:28.580353 26842 net.cpp:100] Creating Layer relu2
I1016 23:26:28.580358 26842 net.cpp:434] relu2 <- conv2
I1016 23:26:28.580364 26842 net.cpp:395] relu2 -> conv2 (in-place)
I1016 23:26:28.580373 26842 net.cpp:150] Setting up relu2
I1016 23:26:28.580389 26842 net.cpp:157] Top shape: 128 256 23 23 (17334272)
I1016 23:26:28.580391 26842 net.cpp:165] Memory required for data: 586856448
I1016 23:26:28.580395 26842 layer_factory.hpp:77] Creating layer pool2
I1016 23:26:28.580415 26842 net.cpp:100] Creating Layer pool2
I1016 23:26:28.580417 26842 net.cpp:434] pool2 <- conv2
I1016 23:26:28.580422 26842 net.cpp:408] pool2 -> pool2
I1016 23:26:28.580456 26842 net.cpp:150] Setting up pool2
I1016 23:26:28.580462 26842 net.cpp:157] Top shape: 128 256 11 11 (3964928)
I1016 23:26:28.580466 26842 net.cpp:165] Memory required for data: 602716160
I1016 23:26:28.580468 26842 layer_factory.hpp:77] Creating layer norm2
I1016 23:26:28.580476 26842 net.cpp:100] Creating Layer norm2
I1016 23:26:28.580479 26842 net.cpp:434] norm2 <- pool2
I1016 23:26:28.580484 26842 net.cpp:408] norm2 -> norm2
I1016 23:26:28.580510 26842 net.cpp:150] Setting up norm2
I1016 23:26:28.580514 26842 net.cpp:157] Top shape: 128 256 11 11 (3964928)
I1016 23:26:28.580518 26842 net.cpp:165] Memory required for data: 618575872
I1016 23:26:28.580520 26842 layer_factory.hpp:77] Creating layer conv3
I1016 23:26:28.580529 26842 net.cpp:100] Creating Layer conv3
I1016 23:26:28.580533 26842 net.cpp:434] conv3 <- norm2
I1016 23:26:28.580538 26842 net.cpp:408] conv3 -> conv3
I1016 23:26:28.601248 26842 net.cpp:150] Setting up conv3
I1016 23:26:28.601277 26842 net.cpp:157] Top shape: 128 384 9 9 (3981312)
I1016 23:26:28.601282 26842 net.cpp:165] Memory required for data: 634501120
I1016 23:26:28.601300 26842 layer_factory.hpp:77] Creating layer relu3
I1016 23:26:28.601316 26842 net.cpp:100] Creating Layer relu3
I1016 23:26:28.601321 26842 net.cpp:434] relu3 <- conv3
I1016 23:26:28.601327 26842 net.cpp:395] relu3 -> conv3 (in-place)
I1016 23:26:28.601336 26842 net.cpp:150] Setting up relu3
I1016 23:26:28.601341 26842 net.cpp:157] Top shape: 128 384 9 9 (3981312)
I1016 23:26:28.601343 26842 net.cpp:165] Memory required for data: 650426368
I1016 23:26:28.601347 26842 layer_factory.hpp:77] Creating layer conv4
I1016 23:26:28.601357 26842 net.cpp:100] Creating Layer conv4
I1016 23:26:28.601361 26842 net.cpp:434] conv4 <- conv3
I1016 23:26:28.601366 26842 net.cpp:408] conv4 -> conv4
I1016 23:26:28.617964 26842 net.cpp:150] Setting up conv4
I1016 23:26:28.617991 26842 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I1016 23:26:28.617996 26842 net.cpp:165] Memory required for data: 660060160
I1016 23:26:28.618006 26842 layer_factory.hpp:77] Creating layer relu4
I1016 23:26:28.618016 26842 net.cpp:100] Creating Layer relu4
I1016 23:26:28.618021 26842 net.cpp:434] relu4 <- conv4
I1016 23:26:28.618028 26842 net.cpp:395] relu4 -> conv4 (in-place)
I1016 23:26:28.618036 26842 net.cpp:150] Setting up relu4
I1016 23:26:28.618041 26842 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I1016 23:26:28.618044 26842 net.cpp:165] Memory required for data: 669693952
I1016 23:26:28.618047 26842 layer_factory.hpp:77] Creating layer conv5
I1016 23:26:28.618058 26842 net.cpp:100] Creating Layer conv5
I1016 23:26:28.618062 26842 net.cpp:434] conv5 <- conv4
I1016 23:26:28.618067 26842 net.cpp:408] conv5 -> conv5
I1016 23:26:28.630290 26842 net.cpp:150] Setting up conv5
I1016 23:26:28.630317 26842 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I1016 23:26:28.630322 26842 net.cpp:165] Memory required for data: 676116480
I1016 23:26:28.630337 26842 layer_factory.hpp:77] Creating layer relu5
I1016 23:26:28.630347 26842 net.cpp:100] Creating Layer relu5
I1016 23:26:28.630352 26842 net.cpp:434] relu5 <- conv5
I1016 23:26:28.630358 26842 net.cpp:395] relu5 -> conv5 (in-place)
I1016 23:26:28.630367 26842 net.cpp:150] Setting up relu5
I1016 23:26:28.630373 26842 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I1016 23:26:28.630376 26842 net.cpp:165] Memory required for data: 682539008
I1016 23:26:28.630379 26842 layer_factory.hpp:77] Creating layer fc6
I1016 23:26:28.630388 26842 net.cpp:100] Creating Layer fc6
I1016 23:26:28.630391 26842 net.cpp:434] fc6 <- conv5
I1016 23:26:28.630398 26842 net.cpp:408] fc6 -> fc6
I1016 23:26:29.819900 26842 net.cpp:150] Setting up fc6
I1016 23:26:29.819941 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:29.819944 26842 net.cpp:165] Memory required for data: 684636160
I1016 23:26:29.819957 26842 layer_factory.hpp:77] Creating layer relu6
I1016 23:26:29.819968 26842 net.cpp:100] Creating Layer relu6
I1016 23:26:29.819973 26842 net.cpp:434] relu6 <- fc6
I1016 23:26:29.819979 26842 net.cpp:395] relu6 -> fc6 (in-place)
I1016 23:26:29.819988 26842 net.cpp:150] Setting up relu6
I1016 23:26:29.819993 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:29.819995 26842 net.cpp:165] Memory required for data: 686733312
I1016 23:26:29.819998 26842 layer_factory.hpp:77] Creating layer drop6
I1016 23:26:29.820005 26842 net.cpp:100] Creating Layer drop6
I1016 23:26:29.820008 26842 net.cpp:434] drop6 <- fc6
I1016 23:26:29.820013 26842 net.cpp:395] drop6 -> fc6 (in-place)
I1016 23:26:29.820029 26842 net.cpp:150] Setting up drop6
I1016 23:26:29.820034 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:29.820037 26842 net.cpp:165] Memory required for data: 688830464
I1016 23:26:29.820040 26842 layer_factory.hpp:77] Creating layer fc7
I1016 23:26:29.820047 26842 net.cpp:100] Creating Layer fc7
I1016 23:26:29.820050 26842 net.cpp:434] fc7 <- fc6
I1016 23:26:29.820055 26842 net.cpp:408] fc7 -> fc7
I1016 23:26:30.195057 26842 net.cpp:150] Setting up fc7
I1016 23:26:30.195089 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:30.195096 26842 net.cpp:165] Memory required for data: 690927616
I1016 23:26:30.195112 26842 layer_factory.hpp:77] Creating layer relu7
I1016 23:26:30.195128 26842 net.cpp:100] Creating Layer relu7
I1016 23:26:30.195133 26842 net.cpp:434] relu7 <- fc7
I1016 23:26:30.195139 26842 net.cpp:395] relu7 -> fc7 (in-place)
I1016 23:26:30.195147 26842 net.cpp:150] Setting up relu7
I1016 23:26:30.195152 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:30.195155 26842 net.cpp:165] Memory required for data: 693024768
I1016 23:26:30.195158 26842 layer_factory.hpp:77] Creating layer drop7
I1016 23:26:30.195164 26842 net.cpp:100] Creating Layer drop7
I1016 23:26:30.195168 26842 net.cpp:434] drop7 <- fc7
I1016 23:26:30.195171 26842 net.cpp:395] drop7 -> fc7 (in-place)
I1016 23:26:30.195188 26842 net.cpp:150] Setting up drop7
I1016 23:26:30.195194 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:30.195196 26842 net.cpp:165] Memory required for data: 695121920
I1016 23:26:30.195200 26842 layer_factory.hpp:77] Creating layer fc8
I1016 23:26:30.195207 26842 net.cpp:100] Creating Layer fc8
I1016 23:26:30.195211 26842 net.cpp:434] fc8 <- fc7
I1016 23:26:30.195215 26842 net.cpp:408] fc8 -> fc8
I1016 23:26:30.286775 26842 net.cpp:150] Setting up fc8
I1016 23:26:30.286805 26842 net.cpp:157] Top shape: 128 1000 (128000)
I1016 23:26:30.286810 26842 net.cpp:165] Memory required for data: 695633920
I1016 23:26:30.286821 26842 layer_factory.hpp:77] Creating layer data_p
I1016 23:26:30.286937 26842 net.cpp:100] Creating Layer data_p
I1016 23:26:30.286955 26842 net.cpp:408] data_p -> data_p
I1016 23:26:30.286967 26842 net.cpp:408] data_p -> label_p
I1016 23:26:30.286973 26842 data_transformer.cpp:25] Loading mean file from: /media/ailab/Data/imagenet/imagenet_mean.binaryproto
I1016 23:26:30.337890 26848 db_lmdb.cpp:35] Opened lmdb /media/ailab/Data/imagenet/ilsvrc12_train2_lmdb
I1016 23:26:30.374174 26842 data_layer.cpp:41] output data size: 128,3,227,227
I1016 23:26:30.610064 26842 net.cpp:150] Setting up data_p
I1016 23:26:30.610098 26842 net.cpp:157] Top shape: 128 3 227 227 (19787136)
I1016 23:26:30.610105 26842 net.cpp:157] Top shape: 128 (128)
I1016 23:26:30.610110 26842 net.cpp:165] Memory required for data: 774782976
I1016 23:26:30.610116 26842 layer_factory.hpp:77] Creating layer conv1_p
I1016 23:26:30.610131 26842 net.cpp:100] Creating Layer conv1_p
I1016 23:26:30.610136 26842 net.cpp:434] conv1_p <- data_p
I1016 23:26:30.610146 26842 net.cpp:408] conv1_p -> conv1_p
I1016 23:26:30.665897 26842 net.cpp:150] Setting up conv1_p
I1016 23:26:30.665932 26842 net.cpp:157] Top shape: 128 96 55 55 (37171200)
I1016 23:26:30.665940 26842 net.cpp:165] Memory required for data: 923467776
I1016 23:26:30.665952 26842 net.cpp:493] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I1016 23:26:30.665961 26842 net.cpp:493] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I1016 23:26:30.665967 26842 layer_factory.hpp:77] Creating layer relu1_p
I1016 23:26:30.665979 26842 net.cpp:100] Creating Layer relu1_p
I1016 23:26:30.665985 26842 net.cpp:434] relu1_p <- conv1_p
I1016 23:26:30.665994 26842 net.cpp:395] relu1_p -> conv1_p (in-place)
I1016 23:26:30.666005 26842 net.cpp:150] Setting up relu1_p
I1016 23:26:30.666013 26842 net.cpp:157] Top shape: 128 96 55 55 (37171200)
I1016 23:26:30.666018 26842 net.cpp:165] Memory required for data: 1072152576
I1016 23:26:30.666023 26842 layer_factory.hpp:77] Creating layer pool1_p
I1016 23:26:30.666030 26842 net.cpp:100] Creating Layer pool1_p
I1016 23:26:30.666035 26842 net.cpp:434] pool1_p <- conv1_p
I1016 23:26:30.666043 26842 net.cpp:408] pool1_p -> pool1_p
I1016 23:26:30.666080 26842 net.cpp:150] Setting up pool1_p
I1016 23:26:30.666088 26842 net.cpp:157] Top shape: 128 96 27 27 (8957952)
I1016 23:26:30.666093 26842 net.cpp:165] Memory required for data: 1107984384
I1016 23:26:30.666098 26842 layer_factory.hpp:77] Creating layer norm1_p
I1016 23:26:30.666106 26842 net.cpp:100] Creating Layer norm1_p
I1016 23:26:30.666110 26842 net.cpp:434] norm1_p <- pool1_p
I1016 23:26:30.666117 26842 net.cpp:408] norm1_p -> norm1_p
I1016 23:26:30.666155 26842 net.cpp:150] Setting up norm1_p
I1016 23:26:30.666173 26842 net.cpp:157] Top shape: 128 96 27 27 (8957952)
I1016 23:26:30.666178 26842 net.cpp:165] Memory required for data: 1143816192
I1016 23:26:30.666183 26842 layer_factory.hpp:77] Creating layer conv2_p
I1016 23:26:30.666194 26842 net.cpp:100] Creating Layer conv2_p
I1016 23:26:30.666200 26842 net.cpp:434] conv2_p <- norm1_p
I1016 23:26:30.666208 26842 net.cpp:408] conv2_p -> conv2_p
I1016 23:26:30.678177 26842 net.cpp:150] Setting up conv2_p
I1016 23:26:30.678221 26842 net.cpp:157] Top shape: 128 256 23 23 (17334272)
I1016 23:26:30.678228 26842 net.cpp:165] Memory required for data: 1213153280
I1016 23:26:30.678237 26842 net.cpp:493] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I1016 23:26:30.678256 26842 net.cpp:493] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I1016 23:26:30.678272 26842 layer_factory.hpp:77] Creating layer relu2_p
I1016 23:26:30.678284 26842 net.cpp:100] Creating Layer relu2_p
I1016 23:26:30.678290 26842 net.cpp:434] relu2_p <- conv2_p
I1016 23:26:30.678299 26842 net.cpp:395] relu2_p -> conv2_p (in-place)
I1016 23:26:30.678309 26842 net.cpp:150] Setting up relu2_p
I1016 23:26:30.678318 26842 net.cpp:157] Top shape: 128 256 23 23 (17334272)
I1016 23:26:30.678321 26842 net.cpp:165] Memory required for data: 1282490368
I1016 23:26:30.678326 26842 layer_factory.hpp:77] Creating layer pool2_p
I1016 23:26:30.678335 26842 net.cpp:100] Creating Layer pool2_p
I1016 23:26:30.678340 26842 net.cpp:434] pool2_p <- conv2_p
I1016 23:26:30.678359 26842 net.cpp:408] pool2_p -> pool2_p
I1016 23:26:30.678414 26842 net.cpp:150] Setting up pool2_p
I1016 23:26:30.678422 26842 net.cpp:157] Top shape: 128 256 11 11 (3964928)
I1016 23:26:30.678427 26842 net.cpp:165] Memory required for data: 1298350080
I1016 23:26:30.678432 26842 layer_factory.hpp:77] Creating layer norm2_p
I1016 23:26:30.678442 26842 net.cpp:100] Creating Layer norm2_p
I1016 23:26:30.678447 26842 net.cpp:434] norm2_p <- pool2_p
I1016 23:26:30.678464 26842 net.cpp:408] norm2_p -> norm2_p
I1016 23:26:30.678511 26842 net.cpp:150] Setting up norm2_p
I1016 23:26:30.678520 26842 net.cpp:157] Top shape: 128 256 11 11 (3964928)
I1016 23:26:30.678524 26842 net.cpp:165] Memory required for data: 1314209792
I1016 23:26:30.678529 26842 layer_factory.hpp:77] Creating layer conv3_p
I1016 23:26:30.678552 26842 net.cpp:100] Creating Layer conv3_p
I1016 23:26:30.678557 26842 net.cpp:434] conv3_p <- norm2_p
I1016 23:26:30.678565 26842 net.cpp:408] conv3_p -> conv3_p
I1016 23:26:30.712787 26842 net.cpp:150] Setting up conv3_p
I1016 23:26:30.712836 26842 net.cpp:157] Top shape: 128 384 9 9 (3981312)
I1016 23:26:30.712846 26842 net.cpp:165] Memory required for data: 1330135040
I1016 23:26:30.712857 26842 net.cpp:493] Sharing parameters 'conv3_w' owned by layer 'conv3', param index 0
I1016 23:26:30.712865 26842 net.cpp:493] Sharing parameters 'conv3_b' owned by layer 'conv3', param index 1
I1016 23:26:30.712872 26842 layer_factory.hpp:77] Creating layer relu3_p
I1016 23:26:30.712894 26842 net.cpp:100] Creating Layer relu3_p
I1016 23:26:30.712903 26842 net.cpp:434] relu3_p <- conv3_p
I1016 23:26:30.712918 26842 net.cpp:395] relu3_p -> conv3_p (in-place)
I1016 23:26:30.712930 26842 net.cpp:150] Setting up relu3_p
I1016 23:26:30.712939 26842 net.cpp:157] Top shape: 128 384 9 9 (3981312)
I1016 23:26:30.712942 26842 net.cpp:165] Memory required for data: 1346060288
I1016 23:26:30.712957 26842 layer_factory.hpp:77] Creating layer conv4_p
I1016 23:26:30.712968 26842 net.cpp:100] Creating Layer conv4_p
I1016 23:26:30.712973 26842 net.cpp:434] conv4_p <- conv3_p
I1016 23:26:30.712991 26842 net.cpp:408] conv4_p -> conv4_p
I1016 23:26:30.738713 26842 net.cpp:150] Setting up conv4_p
I1016 23:26:30.738750 26842 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I1016 23:26:30.738759 26842 net.cpp:165] Memory required for data: 1355694080
I1016 23:26:30.738768 26842 net.cpp:493] Sharing parameters 'conv4_w' owned by layer 'conv4', param index 0
I1016 23:26:30.738775 26842 net.cpp:493] Sharing parameters 'conv4_b' owned by layer 'conv4', param index 1
I1016 23:26:30.738796 26842 layer_factory.hpp:77] Creating layer relu4_p
I1016 23:26:30.738808 26842 net.cpp:100] Creating Layer relu4_p
I1016 23:26:30.738816 26842 net.cpp:434] relu4_p <- conv4_p
I1016 23:26:30.738826 26842 net.cpp:395] relu4_p -> conv4_p (in-place)
I1016 23:26:30.738838 26842 net.cpp:150] Setting up relu4_p
I1016 23:26:30.738845 26842 net.cpp:157] Top shape: 128 384 7 7 (2408448)
I1016 23:26:30.738850 26842 net.cpp:165] Memory required for data: 1365327872
I1016 23:26:30.738855 26842 layer_factory.hpp:77] Creating layer conv5_p
I1016 23:26:30.738869 26842 net.cpp:100] Creating Layer conv5_p
I1016 23:26:30.738875 26842 net.cpp:434] conv5_p <- conv4_p
I1016 23:26:30.738883 26842 net.cpp:408] conv5_p -> conv5_p
I1016 23:26:30.755635 26842 net.cpp:150] Setting up conv5_p
I1016 23:26:30.755671 26842 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I1016 23:26:30.755678 26842 net.cpp:165] Memory required for data: 1371750400
I1016 23:26:30.755687 26842 net.cpp:493] Sharing parameters 'conv5_w' owned by layer 'conv5', param index 0
I1016 23:26:30.755695 26842 net.cpp:493] Sharing parameters 'conv5_b' owned by layer 'conv5', param index 1
I1016 23:26:30.755702 26842 layer_factory.hpp:77] Creating layer relu5_p
I1016 23:26:30.755714 26842 net.cpp:100] Creating Layer relu5_p
I1016 23:26:30.755722 26842 net.cpp:434] relu5_p <- conv5_p
I1016 23:26:30.755730 26842 net.cpp:395] relu5_p -> conv5_p (in-place)
I1016 23:26:30.755741 26842 net.cpp:150] Setting up relu5_p
I1016 23:26:30.755748 26842 net.cpp:157] Top shape: 128 256 7 7 (1605632)
I1016 23:26:30.755754 26842 net.cpp:165] Memory required for data: 1378172928
I1016 23:26:30.755759 26842 layer_factory.hpp:77] Creating layer fc6_p
I1016 23:26:30.755771 26842 net.cpp:100] Creating Layer fc6_p
I1016 23:26:30.755776 26842 net.cpp:434] fc6_p <- conv5_p
I1016 23:26:30.755785 26842 net.cpp:408] fc6_p -> fc6_p
I1016 23:26:32.041352 26842 net.cpp:150] Setting up fc6_p
I1016 23:26:32.041386 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:32.041390 26842 net.cpp:165] Memory required for data: 1380270080
I1016 23:26:32.041398 26842 net.cpp:493] Sharing parameters 'fc6_w' owned by layer 'fc6', param index 0
I1016 23:26:32.041402 26842 net.cpp:493] Sharing parameters 'fc6_b' owned by layer 'fc6', param index 1
I1016 23:26:32.041405 26842 layer_factory.hpp:77] Creating layer relu6_p
I1016 23:26:32.041414 26842 net.cpp:100] Creating Layer relu6_p
I1016 23:26:32.041419 26842 net.cpp:434] relu6_p <- fc6_p
I1016 23:26:32.041424 26842 net.cpp:395] relu6_p -> fc6_p (in-place)
I1016 23:26:32.041434 26842 net.cpp:150] Setting up relu6_p
I1016 23:26:32.041437 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:32.041440 26842 net.cpp:165] Memory required for data: 1382367232
I1016 23:26:32.041443 26842 layer_factory.hpp:77] Creating layer drop6_p
I1016 23:26:32.041450 26842 net.cpp:100] Creating Layer drop6_p
I1016 23:26:32.041452 26842 net.cpp:434] drop6_p <- fc6_p
I1016 23:26:32.041456 26842 net.cpp:395] drop6_p -> fc6_p (in-place)
I1016 23:26:32.041470 26842 net.cpp:150] Setting up drop6_p
I1016 23:26:32.041474 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:32.041477 26842 net.cpp:165] Memory required for data: 1384464384
I1016 23:26:32.041481 26842 layer_factory.hpp:77] Creating layer fc7_p
I1016 23:26:32.041487 26842 net.cpp:100] Creating Layer fc7_p
I1016 23:26:32.041491 26842 net.cpp:434] fc7_p <- fc6_p
I1016 23:26:32.041496 26842 net.cpp:408] fc7_p -> fc7_p
I1016 23:26:32.434033 26842 net.cpp:150] Setting up fc7_p
I1016 23:26:32.434073 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:32.434078 26842 net.cpp:165] Memory required for data: 1386561536
I1016 23:26:32.434084 26842 net.cpp:493] Sharing parameters 'fc7_w' owned by layer 'fc7', param index 0
I1016 23:26:32.434088 26842 net.cpp:493] Sharing parameters 'fc7_b' owned by layer 'fc7', param index 1
I1016 23:26:32.434092 26842 layer_factory.hpp:77] Creating layer relu7_p
I1016 23:26:32.434101 26842 net.cpp:100] Creating Layer relu7_p
I1016 23:26:32.434113 26842 net.cpp:434] relu7_p <- fc7_p
I1016 23:26:32.434126 26842 net.cpp:395] relu7_p -> fc7_p (in-place)
I1016 23:26:32.434134 26842 net.cpp:150] Setting up relu7_p
I1016 23:26:32.434139 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:32.434141 26842 net.cpp:165] Memory required for data: 1388658688
I1016 23:26:32.434144 26842 layer_factory.hpp:77] Creating layer drop7_p
I1016 23:26:32.434151 26842 net.cpp:100] Creating Layer drop7_p
I1016 23:26:32.434154 26842 net.cpp:434] drop7_p <- fc7_p
I1016 23:26:32.434159 26842 net.cpp:395] drop7_p -> fc7_p (in-place)
I1016 23:26:32.434172 26842 net.cpp:150] Setting up drop7_p
I1016 23:26:32.434178 26842 net.cpp:157] Top shape: 128 4096 (524288)
I1016 23:26:32.434180 26842 net.cpp:165] Memory required for data: 1390755840
I1016 23:26:32.434183 26842 layer_factory.hpp:77] Creating layer fc8_p
I1016 23:26:32.434190 26842 net.cpp:100] Creating Layer fc8_p
I1016 23:26:32.434193 26842 net.cpp:434] fc8_p <- fc7_p
I1016 23:26:32.434198 26842 net.cpp:408] fc8_p -> fc8_p
I1016 23:26:32.528389 26842 net.cpp:150] Setting up fc8_p
I1016 23:26:32.528420 26842 net.cpp:157] Top shape: 128 1000 (128000)
I1016 23:26:32.528424 26842 net.cpp:165] Memory required for data: 1391267840
I1016 23:26:32.528431 26842 net.cpp:493] Sharing parameters 'fc8_w' owned by layer 'fc8', param index 0
I1016 23:26:32.528436 26842 net.cpp:493] Sharing parameters 'fc8_b' owned by layer 'fc8', param index 1
I1016 23:26:32.528439 26842 layer_factory.hpp:77] Creating layer same_not_same_label
F1016 23:26:32.528467 26842 layer_factory.hpp:81] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Python (known types: AbsVal, Accuracy, ArgMax, BNLL, BatchNorm, BatchReindex, Bias, Concat, ContrastiveLoss, Convolution, Crop, Data, Deconvolution, Dropout, DummyData, ELU, Eltwise, Embed, EuclideanLoss, Exp, Filter, Flatten, HDF5Data, HDF5Output, HingeLoss, Im2col, ImageData, InfogainLoss, InnerProduct, Input, LRN, LSTM, LSTMUnit, Log, MVN, MemoryData, MultinomialLogisticLoss, PReLU, Parameter, Pooling, Power, RNN, ReLU, Reduction, Reshape, SPP, Scale, Sigmoid, SigmoidCrossEntropyLoss, Silence, Slice, Softmax, SoftmaxWithLoss, Split, TanH, Threshold, Tile, WindowData)
*** Check failure stack trace: ***
    @     0x7fbe2515fdaa  (unknown)
    @     0x7fbe2515fce4  (unknown)
    @     0x7fbe2515f6e6  (unknown)
    @     0x7fbe25162687  (unknown)
    @     0x7fbe256ddb5f  caffe::LayerRegistry<>::CreateLayer()
    @     0x7fbe257f2c1c  caffe::Net<>::Init()
    @     0x7fbe257f4d55  caffe::Net<>::Net()
    @     0x7fbe257d992a  caffe::Solver<>::InitTrainNet()
    @     0x7fbe257dab3c  caffe::Solver<>::Init()
    @     0x7fbe257dae6a  caffe::Solver<>::Solver()
    @     0x7fbe257bfa53  caffe::Creator_SGDSolver<>()
    @           0x411d36  caffe::SolverRegistry<>::CreateSolver()
    @           0x40abe2  train()
    @           0x4086bc  main
    @     0x7fbe23ef0f45  (unknown)
    @           0x408f2b  (unknown)
    @              (nil)  (unknown)
